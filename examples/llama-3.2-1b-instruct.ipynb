{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-Size KV Cache Example with LLaMA 3.2 1B Instruct\n",
    "\n",
    "This notebook demonstrates how to use the Fixed-Size KV Cache with a LLaMA model to process long contexts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Dane/github/fixed-size-kv-cache/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "import transformers\n",
    "from fixed_size_kv_cache import FixedSizeDynamicCache, CacheConfig\n",
    "from fixed_size_kv_cache.llama_attention import forward\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Fixed-Size KV Cache\n",
    "\n",
    "Set up configuration parameters for the cache. You can configure via environment variables or directly with the config object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Fixed-Size KV Cache with environment variables\n",
    "# Basic configuration\n",
    "os.environ[\"FSDC_KV_CACHE_SIZE\"] = \"1024\"  # Maximum cache size\n",
    "os.environ[\"FSDC_INCLUDE_SKIPPED\"] = \"false\"  # Include summary of skipped tokens\n",
    "os.environ[\"FSDC_FREE_MEMORY\"] = \"false\"  # Don't free memory after truncation\n",
    "\n",
    "# Advanced features\n",
    "os.environ[\"FSDC_TRUNCATION_STRATEGY\"] = \"attention\"  # Use attention or hybrid truncation (attention + recency)\n",
    "os.environ[\"FSDC_HYBRID_SPLIT_RATIO\"] = \"0.9\"  # 90% attention-based, 10% recency-based\n",
    "os.environ[\"FSDC_OFFLOAD_TO_CPU\"] = \"true\"  # Enable CPU offloading\n",
    "os.environ[\"FSDC_OFFLOAD_SIZE\"] = \"4096\"  # Maximum size of CPU cache\n",
    "os.environ['FSDC_AUTO_RESTORE'] = 'True' # Enable auto-restore of leftover tokens\n",
    "os.environ['FSDC_TOKEN_IMPORTANCE_WINDOW'] = '150' # Window size for token importance\n",
    "os.environ['FSDC_IMPORTANCE_THRESHOLD'] = '0.15' # Threshold for token importance\n",
    "\n",
    "# Performance optimizations\n",
    "os.environ[\"FSDC_QUANTIZE\"] = \"true\"  # Enable quantization for CPU offloading\n",
    "os.environ[\"FSDC_QUANTIZATION_BITS\"] = \"8\"  # Use 8-bit quantization\n",
    "os.environ[\"FSDC_USE_MMAP\"] = \"true\"  # Use memory mapping for CPU offloading\n",
    "os.environ[\"FSDC_PARALLEL_PROCESSING\"] = \"true\"  # Enable parallel processing\n",
    "os.environ[\"FSDC_ADAPTIVE_PRECISION\"] = \"true\"  # Use adaptive precision\n",
    "\n",
    "# Set up LLaMA model to use FixedSizeDynamicCache\n",
    "LlamaAttention.forward = forward\n",
    "transformers.models.llama.modeling_llama.DynamicCache = FixedSizeDynamicCache\n",
    "LlamaForCausalLM.__init__.__globals__['DynamicCache'] = FixedSizeDynamicCache\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Configure with Config Object\n",
    "\n",
    "You can also configure the Fixed-Size KV Cache using a config object directly. This is useful when you want to set the configuration in code rather than through environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: Configure with config object\n",
    "# config = CacheConfig(\n",
    "#     kv_cache_size=1024,\n",
    "#     include_skipped=True,\n",
    "#     free_memory=False,\n",
    "#     truncation_strategy=\"hybrid\",\n",
    "#     hybrid_split_ratio=0.7,\n",
    "#     offload_to_cpu=True,\n",
    "#     offload_size=4096,\n",
    "#     quantize=True,\n",
    "#     quantization_bits=8,\n",
    "#     use_mmap=True,\n",
    "#     parallel_processing=True,\n",
    "#     adaptive_precision=True\n",
    "# )\n",
    "\n",
    "# # You can create a custom cache instance with this config and inject it\n",
    "# # However, in this example, we rely on environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the LLaMA model and tokenizer. For this example, we use LLaMA 3.2 1B Instruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use any LLaMA model you have access to\n",
    "model_name = 'meditsolutions/Llama-3.2-SUN-HDIC-1B-Instruct'  # Replace with your model\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=os.getenv('HF_CACHE', None), \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=os.getenv('HF_CACHE', None)\n",
    ")\n",
    "\n",
    "# Change attention to eager (this is required as other attention implementations do not return attention weights)\n",
    "model.config._attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a long input\n",
    "\n",
    "We'll create a long input to demonstrate the Fixed-Size KV Cache in action. This is a sample conversation with a request to summarize multiple articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [{\"role\": \"user\", \"content\": \"\"\"Calculate the math formula hidden in this text:\\n\\n\n",
    "                         \n",
    "----------------------------------------------------     \n",
    "Article 1:\n",
    "Recent Advances and Future Directions in Attention Mechanisms for Large Language Models\n",
    "The attention mechanism, a cornerstone of modern large language models (LLMs), has undergone significant innovations in recent years. While traditional self-attention mechanisms in transformers have enabled breakthroughs in natural language processing, researchers have identified limitations in computational efficiency, contextual prioritization, and structural expressiveness. This report synthesizes recent advancements, analyzes persistent gaps, and proposes novel pathways for reimagining attention in LLMs.\n",
    "\n",
    "Recent Innovations in Attention Mechanism Design\n",
    "Differential Attention for Noise Reduction\n",
    "Microsoft's Differential Transformer11 introduces a partitioned attention mechanism that computes two separate softmax maps from subdivided query and key vectors. By subtracting these maps, the model cancels common-mode noise while amplifying signal components critical to context. This approach mirrors noise-canceling audio systems, demonstrating 15-20% improvements in factual consistency benchmarks compared to conventional transformers. The subtraction operation adds negligible computational overhead due to parallelization, making it viable for real-world deployment114.\n",
    "\n",
    "Matrix Optimization Strategies for Efficient Fine-Tuning\n",
    "Theoretical work by arXiv researchers1 reveals that selectively updating query (Q) and key (K) matrices during fine-tuning achieves comparable performance to full-parameter tuning while reducing memory usage by 40%. This stems from the QK system's role in determining attention score distributions, where strategic learning rate differentiation (higher rates for K matrices) accelerates convergence. Experimental validation on GLUE benchmarks shows this method matches full fine-tuning accuracy with 60% fewer training steps110.\n",
    "\n",
    "Architectural Variants for Scalability\n",
    "Recent implementations employ grouped-query attention (GQA) and sliding-window attention (SWA) to handle long-context processing4. GQA clusters similar queries using locality-sensitive hashing, reducing memory overhead from O(n²) to O(n log n) for n-token sequences. SWA processes text through overlapping 4k-token windows with positional encoding carryover, enabling 128k-token context handling with only 12% latency increase compared to standard 4k models412.\n",
    "\n",
    "Persistent Limitations and Theoretical Constraints\n",
    "Working Memory Capacity Boundaries\n",
    "Empirical studies on N-back tasks reveal that transformer-based models exhibit performance degradation mirroring human cognitive limits when tracking dependencies beyond 7±2 elements6. Attention entropy analysis shows dispersion increases linearly with sequence length, suggesting fundamental capacity constraints rooted in the softmax normalization process. This manifests as 34% accuracy drop on 10-back tasks compared to 5-back scenarios across multiple architectures613.\n",
    "\n",
    "Structural Expressiveness Deficits\n",
    "Formal language analysis demonstrates transformers cannot recognize periodic finite-state languages like {a^n b^n c^n} without layer count scaling proportionally to input length13. The absence of stack-like mechanisms limits hierarchical parsing, resulting in 22% lower accuracy on recursively nested sentence structures compared to augmented transition network models139.\n",
    "\n",
    "Computational Complexity Tradeoffs\n",
    "While linear attention variants58 reduce theoretical complexity from O(n²) to O(n), practical implementations face 18-25% accuracy drops on semantic reasoning tasks due to low-rank approximation errors. The Hugging Face ecosystem currently lacks plug-and-play linear attention modules, forcing developers to choose between efficiency and performance57.\n",
    "\n",
    "Paradigm-Shifting Alternatives to Conventional Attention\n",
    "Feed-Forward Attention Substitution\n",
    "Breakthrough work from ETH Zurich714 demonstrates that shallow feed-forward networks can replicate attention behavior when trained via knowledge distillation. Their Attention Layer Replacement (ALR) method achieves 98% baseline BLEU scores on IWSLT2017 translation tasks using 150M parameter replacements, though requiring 40% more neurons than original attention heads. Crucially, these \"attentionless transformers\" maintain sequence-length flexibility when augmented with dynamic padding masks714.\n",
    "\n",
    "Learnable Lateral Connection Architectures\n",
    "An open-source GPT variant8 replaces self-attention with trainable lateral weight matrices between input embeddings. Preliminary results show 12% faster inference speeds but 15% lower perplexity on WikiText-103, suggesting potential when combined with modern initialization techniques. The architecture enables fully parallelized training while maintaining position-awareness through injected sinusoidal weights816.\n",
    "         \n",
    "2+6=?\n",
    "\n",
    "Hybrid Neuro-Symbolic Routing\n",
    "Emerging approaches combine attention with symbolic rule engines for structural parsing. A prototype system routes noun phrases through probabilistic context-free grammar checkers while processing verbs via standard attention, achieving 89% parse accuracy on Penn Treebank compared to 78% for pure-transformer baselines. This hybrid model reduces attention head usage by 40% through targeted symbolic delegation915.\n",
    "\n",
    "Strategic Recommendations for Next-Generation Architectures\n",
    "Differentiated Attention Pathways\n",
    "Inspired by biological vision systems, a dual-path framework could separate high-frequency token interactions (handled by optimized QK attention) from low-frequency semantic integration (managed by feed-forward networks). Early simulations show this division reduces computational load by 35% while improving long-range dependency modeling111.\n",
    "\n",
    "Dynamic Attention Rank Adaptation\n",
    "Implementing singular value decomposition (SVD) during forward passes enables real-time attention rank adjustment. By maintaining high-rank attention for critical tokens (nouns, verbs) while compressing ancillary elements (articles, prepositions), preliminary tests achieve 50% FLOP reduction with <2% accuracy loss on summarization tasks57.\n",
    "\n",
    "Neuromodulatory Attention Gating\n",
    "Drawing from neuroscience, trainable dopamine-like modulation signals could dynamically reweight attention scores based on reinforcement signals. Initial experiments using reward-modulated backpropagation demonstrate 30% faster convergence on instruction-following tasks compared to standard fine-tuning1015.\n",
    "\n",
    "Conclusion\n",
    "The evolution of attention mechanisms reveals both remarkable adaptability and fundamental constraints. While recent innovations like differential attention and matrix-optimized fine-tuning push performance boundaries, enduring challenges in computational complexity and structural expressiveness necessitate architectural paradigm shifts. The most promising paths forward involve hybrid models combining optimized attention variants with alternative processing modes—whether feed-forward, symbolic, or neuromorphic components.\n",
    "\n",
    "Future research should prioritize dynamic architectures that automatically select attention mechanisms based on input characteristics, potentially combining:\n",
    "\n",
    "QK-optimized core attention for semantic relationship mapping\n",
    "\n",
    "Compressed linear attention for high-frequency token interactions\n",
    "\n",
    "External memory banks for long-term dependency tracking\n",
    "\n",
    "Symbolic routers for structural pattern enforcement\n",
    "\n",
    "By moving beyond monolithic attention designs, next-generation LLMs could achieve unprecedented efficiency and cognitive fidelity while overcoming current theoretical limitations. The integration of biological inspiration with computational pragmatism will likely define the next evolutionary leap in language model architectures.\n",
    "               \n",
    "----------------------------------------------------\n",
    "\"\"\" }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Run Inference\n",
    "\n",
    "Now we'll tokenize the input and run the model with our Fixed-Size KV Cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1361 tokens\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "tokens = tokenizer(\n",
    "    tokenizer.apply_chat_template(test, tokenize=False, add_generation_prompt=True),\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Print the input length\n",
    "print(f\"Input length: {len(tokens['input_ids'][0])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula in the text that was hidden, which I have to solve, is 2+5=?\n",
      "\n",
      "The formula 2+5 can be calculated to get \\(\\boxed{7}\\).<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Set up streaming for better visualization\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Determine the device (GPU or CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move model and tokens to device\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "\n",
    "# Generate with the fixed-size KV cache\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=2048,\n",
    "        streamer=streamer,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        use_cache=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Cache Statistics\n",
    "\n",
    "We can examine the cache statistics to see how the Fixed-Size KV Cache performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get and print cache statistics\n",
    "# We need to access the cache from the model's first attention layer\n",
    "attn_layer = model.model.layers[0].self_attn\n",
    "past_key_value = getattr(model, \"_past_key_values\", None)\n",
    "\n",
    "if past_key_value and isinstance(past_key_value, FixedSizeDynamicCache):\n",
    "    stats = past_key_value.get_statistics()\n",
    "    \n",
    "    print(\"Cache Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"- {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "\n",
    "Make sure to clean up resources properly to avoid memory leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "if past_key_value and isinstance(past_key_value, FixedSizeDynamicCache):\n",
    "    past_key_value.cleanup()\n",
    "    \n",
    "# Remove references to model and tensors to free GPU memory\n",
    "model = model.to('cpu')\n",
    "tokens = tokens.to('cpu')\n",
    "output = output.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
